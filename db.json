{
  "models": [
    {
      "author": "BigCode",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659521200179-5e48005437cb5b49818287a5.png?w=200&h=200&f=face",
      "downloads": 2427,
      "likes": 247,
      "model": "starcoder2-15b",
      "tag": "text-generation"
    },
    {
      "author": "Stability AI",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png?w=200&h=200&f=face",
      "downloads": 4433425,
      "likes": 4593,
      "model": "stable-diffusion-xl-base-1.0",
      "tag": "text-to-image"
    },
    {
      "author": "Brandon G. Neri",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669957883411-6303f37c3926de1f7ec42d3e.jpeg?w=200&h=200&f=face",
      "downloads": 205,
      "likes": 44,
      "model": "pixelcascade128-v0.1",
      "tag": "text-to-image"
    },
    {
      "author": "Playground",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62c627c4644269e788cfee34/mV0Cwic2o92LNKiFBbraJ.png?w=200&h=200&f=face",
      "downloads": 27456,
      "likes": 235,
      "model": "playground-v2.5-1024px-aesthetic",
      "tag": "text-to-image"
    },
    {
      "author": "Microsoft",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png?w=200&h=200&f=face",
      "downloads": 444373,
      "likes": 2865,
      "model": "phi-2",
      "tag": "text-generation"
    },
    {
      "author": "Stability AI",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png?w=200&h=200&f=face",
      "downloads": 11411,
      "likes": 381,
      "model": "stable-video-diffusion-img2vid-xt-1-1",
      "tag": "image-to-video"
    },
    {
      "author": "Meta Llama 2",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646cf8084eefb026fb8fd8bc/oCTqufkdTkjyGodsx1vo1.png?w=200&h=200&f=face",
      "downloads": 1203921,
      "likes": 2901,
      "model": "Llama-2-7b-chat-hf",
      "tag": "text-generation"
    },
    {
      "author": "Mistralai",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62dac1c7a8ead43d20e3e17a/wrLf5yaGC6ng4XME70w6Z.png?w=200&h=200&f=face",
      "downloads": 1056067,
      "likes": 3131,
      "model": "Mixtral-8x7B-Instruct-v0.1",
      "tag": "text-generation"
    }
  ],
  "starcoder2-15b": {
    "content": [
      {
        "title": "Description",
        "text": "The StarCoder2-15B model, developed by the BigCode project, is a powerful text generation model trained on a vast array of programming languages sourced from platforms like GitHub, Arxiv, and Wikipedia. With its impressive 15 billion parameters, this model offers extensive capabilities in generating code snippets and text relevant to programming tasks and queries."
      },
      {
        "title": "Example Usage",
        "text": "To demonstrate its usage, consider the following code snippet:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/starcoder2-15b\"\ndevice = \"cuda\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```"
      },
      {
        "title": "Potential Use Cases",
        "text": "The StarCoder2-15B model finds applications in a wide range of scenarios within the programming domain. It can assist developers by providing code snippets, suggesting completions, or even generating documentation. Additionally, it can aid in code summarization, translation between programming languages, and code quality assessment. Its versatility and vast training data make it a valuable tool for enhancing developer productivity and efficiency."
      },
      {
        "title": "Model Architecture and Training",
        "text": "This model's architecture incorporates advanced techniques such as Grouped Query Attention and sliding window attention, allowing it to handle large-scale text generation tasks efficiently. It was trained using the Fill-in-the-Middle objective on a staggering 4+ trillion tokens, utilizing NVIDIA NeMo Framework and the powerful NVIDIA Eos Supercomputer."
      },
      {
        "title": "License and Citation",
        "text": "For developers and researchers interested in leveraging the StarCoder2-15B model, it is licensed under the BigCode OpenRAIL-M v1 license agreement. Detailed information about its training process, hardware setup, and performance metrics can be found in the associated paper, 'StarCoder 2 and The Stack v2: The Next Generation'."
      }
    ]
  }
}
