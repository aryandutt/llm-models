{
  "models": [
    {
      "author": "BigCode",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659521200179-5e48005437cb5b49818287a5.png?w=200&h=200&f=face",
      "downloads": 2427,
      "likes": 247,
      "model": "starcoder2-15b",
      "tag": "text-generation"
    },
    {
      "author": "Stability AI",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png?w=200&h=200&f=face",
      "downloads": 4433425,
      "likes": 4593,
      "model": "stable-diffusion-xl-base-1.0",
      "tag": "text-to-image"
    },
    {
      "author": "Brandon G. Neri",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669957883411-6303f37c3926de1f7ec42d3e.jpeg?w=200&h=200&f=face",
      "downloads": 205,
      "likes": 44,
      "model": "pixelcascade128-v0.1",
      "tag": "text-to-image"
    },
    {
      "author": "Playground",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62c627c4644269e788cfee34/mV0Cwic2o92LNKiFBbraJ.png?w=200&h=200&f=face",
      "downloads": 27456,
      "likes": 235,
      "model": "playground-v2.5-1024px-aesthetic",
      "tag": "text-to-image"
    },
    {
      "author": "Microsoft",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png?w=200&h=200&f=face",
      "downloads": 444373,
      "likes": 2865,
      "model": "phi-2",
      "tag": "text-generation"
    },
    {
      "author": "Stability AI",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png?w=200&h=200&f=face",
      "downloads": 11411,
      "likes": 381,
      "model": "stable-video-diffusion-img2vid-xt-1-1",
      "tag": "image-to-video"
    },
    {
      "author": "Meta Llama 2",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646cf8084eefb026fb8fd8bc/oCTqufkdTkjyGodsx1vo1.png?w=200&h=200&f=face",
      "downloads": 1203921,
      "likes": 2901,
      "model": "Llama-2-7b-chat-hf",
      "tag": "text-generation"
    },
    {
      "author": "Mistralai",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62dac1c7a8ead43d20e3e17a/wrLf5yaGC6ng4XME70w6Z.png?w=200&h=200&f=face",
      "downloads": 1056067,
      "likes": 3131,
      "model": "Mixtral-8x7B-Instruct-v0.1",
      "tag": "text-generation"
    }
  ],
  "starcoder2-15b": {
    "content": [
      {
        "title": "Description",
        "text": "The StarCoder2-15B model, developed by the BigCode project, is a powerful text generation model trained on a vast array of programming languages sourced from platforms like GitHub, Arxiv, and Wikipedia. With its impressive 15 billion parameters, this model offers extensive capabilities in generating code snippets and text relevant to programming tasks and queries."
      },
      {
        "title": "Example Usage",
        "text": "To demonstrate its usage, consider the following code snippet:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/starcoder2-15b\"\ndevice = \"cuda\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```"
      },
      {
        "title": "Potential Use Cases",
        "text": "The StarCoder2-15B model finds applications in a wide range of scenarios within the programming domain. It can assist developers by providing code snippets, suggesting completions, or even generating documentation. Additionally, it can aid in code summarization, translation between programming languages, and code quality assessment. Its versatility and vast training data make it a valuable tool for enhancing developer productivity and efficiency."
      },
      {
        "title": "Model Architecture and Training",
        "text": "This model's architecture incorporates advanced techniques such as Grouped Query Attention and sliding window attention, allowing it to handle large-scale text generation tasks efficiently. It was trained using the Fill-in-the-Middle objective on a staggering 4+ trillion tokens, utilizing NVIDIA NeMo Framework and the powerful NVIDIA Eos Supercomputer."
      },
      {
        "title": "License and Citation",
        "text": "For developers and researchers interested in leveraging the StarCoder2-15B model, it is licensed under the BigCode OpenRAIL-M v1 license agreement. Detailed information about its training process, hardware setup, and performance metrics can be found in the associated paper, 'StarCoder 2 and The Stack v2: The Next Generation'."
      }
    ]
  },
  "pixelcascade128-v0.1": {
    "content": [
      {
        "title": "Description",
        "text": "The PixelCascade128 v0.1 model, developed by StabilityAI, is an early version designed for generating pixel art using the LoRa for Stable Cascade Stace C framework. This version of the model is capable of creating pixelated images, although it should be noted that it may not produce pixel-perfect or grid-aligned output."
      },
      {
        "title": "Example Usage",
        "text": "To demonstrate its usage, consider the following steps:\n\n- Ensure you have ComfyUI installed and select one of the provided workflows (txt2img or img2img).\n- Make sure to use the correct Stable Cascade files (Stage A, B, and C, effnet_encoder for img2img).\n- Optimal results are obtained with Stage C/B BF16 weights.\n- To force a pixel art style, include the 'pixel art' keyword in your prompt.\n- Removing simple backgrounds can be achieved by using the 'white background' on a negative prompt.\n- Downscale x8 times with nearest neighbors or use Astropulse's Pixel Detector for achieving pixel-perfect results.\n- The model works best for 2048x2048 outputs, with optimal results obtained from img2img with 1024x1024 samples.\n- Use Euler a sampler with 20 steps for Stage C and 10 steps for Stage B for best results.\n- Note that 2048x2048 samples might generate distorted images, so using img2img with 0.7 strength is recommended.\n\nHere's an example code snippet demonstrating the usage of PixelCascade128 v0.1 model:\n\n```python\nimport torch\nfrom diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline\n\ndevice = 'cuda'\nnum_images_per_prompt = 2\n\nprior = StableCascadePriorPipeline.from_pretrained('stabilityai/stable-cascade-prior', torch_dtype=torch.bfloat16).to(device)\ndecoder = StableCascadeDecoderPipeline.from_pretrained('stabilityai/stable-cascade', torch_dtype=torch.float16).to(device)\n\nprompt = 'Anthropomorphic cat dressed as a pilot'\nnegative_prompt = ''\n\nprior_output = prior(\n    prompt=prompt,\n    height=1024,\n    width=1024,\n    negative_prompt=negative_prompt,\n    guidance_scale=4.0,\n    num_images_per_prompt=num_images_per_prompt,\n    num_inference_steps=20\n)\ndecoder_output = decoder(\n    image_embeddings=prior_output.image_embeddings.half(),\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    guidance_scale=0.0,\n    output_type='pil',\n    num_inference_steps=10\n).images\n```"
      },
      {
        "title": "Potential Use Cases",
        "text": "The PixelCascade128 v0.1 model can be utilized in various scenarios related to pixel art creation. It can assist artists and designers in generating pixel art assets, creating retro-style graphics, or adding visual elements to games and animations. Additionally, it can be used for educational purposes to teach pixel art techniques or as a tool for generating unique avatars and icons."
      }
    ]
  },
  "phi-2": {
    "content": [
      {
        "title": "Description",
        "text": "The Phi-2 model, developed by Microsoft, is a Transformer-based model with 2.7 billion parameters. Trained on a combination of data sources including NLP synthetic texts, filtered websites, and common programming languages, Phi-2 offers extensive capabilities in generating text and code relevant to various tasks and queries in natural language processing and programming domains."
      },
      {
        "title": "Example Usage",
        "text": "To demonstrate its usage, consider the following code snippet:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device('cuda')\n\nmodel = AutoModelForCausalLM.from_pretrained('microsoft/phi-2', torch_dtype='auto', trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained('microsoft/phi-2', trust_remote_code=True)\n\ninputs = tokenizer('''def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"\n''', return_tensors='pt', return_attention_mask=False)\n\noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\n```"
      },
      {
        "title": "Intended Uses",
        "text": "Given the diverse training data, the Phi-2 model is best suited for prompts in the QA, chat, and code formats. Users can utilize Phi-2 to generate responses to questions, engage in conversational dialogue, or produce code snippets for various programming tasks."
      },
      {
        "title": "Limitations of Phi-2",
        "text": "Phi-2, like any other AI model, has certain limitations:\n\n- **Generate Inaccurate Code and Facts:** The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions rather than definitive solutions.\n- **Limited Scope for Code:** Majority of Phi-2's training data is based in Python and uses common packages. If the model generates code utilizing other packages or scripts in other languages, manual verification is recommended."
      },
      {
        "title": "Training",
        "text": "Phi-2 was trained using a large dataset and advanced techniques:\n\n- **Architecture:** Transformer-based model with next-word prediction objective\n- **Context Length:** 2048 tokens\n- **Dataset Size:** 250 billion tokens, consisting of NLP synthetic data and filtered web data\n- **Training Tokens:** 1.4 trillion tokens\n- **GPUs:** 96xA100-80G\n- **Training Time:** 14 days\n\nSoftware used during training includes PyTorch, DeepSpeed, and Flash-Attention."
      },
      {
        "title": "License",
        "text": "The Phi-2 model is licensed under the MIT license."
      }
    ]
  }
}
