{
  "models": [
    {
      "author": "BigCode",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659521200179-5e48005437cb5b49818287a5.png?w=200&h=200&f=face",
      "downloads": 2427,
      "likes": 247,
      "model": "starcoder2-15b",
      "tag": "text-generation"
    },
    {
      "author": "Stability AI",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png?w=200&h=200&f=face",
      "downloads": 4433425,
      "likes": 4593,
      "model": "stable-diffusion-xl-base-1.0",
      "tag": "text-to-image"
    },
    {
      "author": "Brandon G. Neri",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669957883411-6303f37c3926de1f7ec42d3e.jpeg?w=200&h=200&f=face",
      "downloads": 205,
      "likes": 44,
      "model": "pixelcascade128-v0.1",
      "tag": "text-to-image"
    },
    {
      "author": "Playground",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62c627c4644269e788cfee34/mV0Cwic2o92LNKiFBbraJ.png?w=200&h=200&f=face",
      "downloads": 27456,
      "likes": 235,
      "model": "playground-v2.5-1024px-aesthetic",
      "tag": "text-to-image"
    },
    {
      "author": "Microsoft",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png?w=200&h=200&f=face",
      "downloads": 444373,
      "likes": 2865,
      "model": "phi-2",
      "tag": "text-generation"
    },
    {
      "author": "Stability AI",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png?w=200&h=200&f=face",
      "downloads": 11411,
      "likes": 381,
      "model": "stable-video-diffusion-img2vid-xt-1-1",
      "tag": "image-to-video"
    },
    {
      "author": "Meta Llama 2",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646cf8084eefb026fb8fd8bc/oCTqufkdTkjyGodsx1vo1.png?w=200&h=200&f=face",
      "downloads": 1203921,
      "likes": 2901,
      "model": "Llama-2-7b-chat-hf",
      "tag": "text-generation"
    },
    {
      "author": "Mistralai",
      "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62dac1c7a8ead43d20e3e17a/wrLf5yaGC6ng4XME70w6Z.png?w=200&h=200&f=face",
      "downloads": 1056067,
      "likes": 3131,
      "model": "Mixtral-8x7B-Instruct-v0.1",
      "tag": "text-generation"
    }
  ],
  "starcoder2-15b": {
    "content": "---\npipeline_tag: text-generation\ninference: true\nwidget:\n- text: 'def print_hello_world():'\n  example_title: Hello world\n  group: Python\ndatasets:\n- bigcode/the-stack-v2-train\nlicense: bigcode-openrail-m\nlibrary_name: transformers\ntags:\n- code\nmodel-index:\n- name: starcoder2-15b\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      name: CruxEval-I\n      type: cruxeval-i\n    metrics:\n    - type: pass@1\n      value: 48.1\n  - task:\n      type: text-generation\n    dataset:\n      name: DS-1000\n      type: ds-1000\n    metrics:\n    - type: pass@1\n      value: 33.8\n  - task:\n      type: text-generation\n    dataset:\n      name: GSM8K (PAL)\n      type: gsm8k-pal\n    metrics:\n    - type: accuracy\n      value: 65.1\n  - task:\n      type: text-generation\n    dataset:\n      name: HumanEval+\n      type: humanevalplus\n    metrics:\n    - type: pass@1\n      value: 37.8\n  - task:\n      type: text-generation\n    dataset:\n      name: HumanEval\n      type: humaneval\n    metrics:\n    - type: pass@1\n      value: 46.3\n  - task:\n      type: text-generation\n    dataset:\n      name: RepoBench-v1.1\n      type: repobench-v1.1\n    metrics:\n    - type: edit-smiliarity\n      value: 74.08\n---\n\n# StarCoder2\n\n<center>\n    <img src=\"https://huggingface.co/datasets/bigcode/admin_private/resolve/main/starcoder2_banner.png\" alt=\"SC2\" width=\"900\" height=\"600\">\n</center>\n\n##  Table of Contents\n\n1. [Model Summary](#model-summary)\n2. [Use](#use)\n3. [Limitations](#limitations)\n4. [Training](#training)\n5. [License](#license)\n6. [Citation](#citation)\n\n## Model Summary\n\nStarCoder2-15B model is a 15B parameter model trained on 600+ programming languages from [The Stack v2](https://huggingface.co/datasets/bigcode/the-stack-v2-train), with opt-out requests excluded. The model uses [Grouped Query Attention](https://arxiv.org/abs/2305.13245), [a context window of 16,384 tokens](https://arxiv.org/abs/2205.14135) with [a sliding window attention of 4,096 tokens](https://arxiv.org/abs/2004.05150v2),  and was trained using the [Fill-in-the-Middle objective](https://arxiv.org/abs/2207.14255) on 4+ trillion tokens.  \nThe model was trained with [NVIDIA NeMoâ„¢ Framework](https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/) using the [NVIDIA Eos Supercomputer](https://blogs.nvidia.com/blog/eos/) built with [NVIDIA DGX H100](https://www.nvidia.com/en-us/data-center/dgx-h100/) systems.\n\n- **Project Website:** [bigcode-project.org](https://www.bigcode-project.org)\n- **Paper:** [Link](https://huggingface.co/papers/2402.19173)\n- **Point of Contact:** [contact@bigcode-project.org](mailto:contact@bigcode-project.org)\n- **Languages:** 600+ Programming languages\n\n## Use\n\n### Intended use\n\nThe model was trained on GitHub code as well as additional selected data sources such as Arxiv and Wikipedia. As such it is _not_ an instruction model and commands like \"Write a function that computes the square root.\" do not work well.\n\n### Generation\nHere are some examples to get started with the model. You can find a script for fine-tuning in StarCoder2's [GitHub repository](https://github.com/bigcode-project/starcoder2).\n\nFirst, make sure to install `transformers` from source:\n```bash\npip install git+https://github.com/huggingface/transformers.git\n```\n\n#### Running the model on CPU/GPU/multi GPU\n* _Using full precision_\n```python\n# pip install git+https://github.com/huggingface/transformers.git # TODO: merge PR to main\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/starcoder2-15b\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n\n* _Using `torch.bfloat16`_\n```python\n# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ncheckpoint = \"bigcode/starcoder2-15b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n```bash\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 32251.33 MB\n```\n\n#### Quantized Versions through `bitsandbytes`\n* _Using 8-bit precision (int8)_\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# to use 4bit use `load_in_4bit=True` instead\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ncheckpoint = \"bigcode/starcoder2-15b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n```bash\n>>> print(f\"Memory footprint: {model.get_memory"
  }
}
